{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Verrakk concentrates on two areas where expert knowledge is applied:\n",
    "    - The BF method\n",
    "    - The insertion of prior knowledge about individual development factors in the CL method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stochastic Models for the CL Method\n",
    "\n",
    "### Mack's Model\n",
    "\n",
    "$$E[D_{i,j}] = \\lambda_jD_{1,j-1}$$\n",
    "$$Var(D_{i,j}) = \\sigma_j^2D_{i,j-1}$$\n",
    "\n",
    "##### Advantage\n",
    "- Parameter estimates and prediction errors can be obtained using a spreadsheet. (it's simple)\n",
    "\n",
    "##### Disadvantages\n",
    "- There is no predictive distribution\n",
    "- Additional parameters must be estimated in order to calculate the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ODP Model\n",
    "\n",
    "$$E[C_{i,j}] = x_iy_j$$\n",
    "$$Var(C_{i,j}) = \\varphi x_iy_j$$\n",
    "\n",
    "where, $x_i$ is expected ult. losses and $y_j$ is ratio of loss emergence.\n",
    "\n",
    "- The reserve estimates are same as the CL method.\n",
    "\n",
    "##### Disadvantages\n",
    "- It requires the column and row sums of incremental values to be positive.\n",
    "- It's hard to see the connection to the CL method from the formulation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Over-dispersed Negative Binomial Model\n",
    "\n",
    "$$E[D_{i,j}] = \\lambda_j D_{i,j-1} \\text{ and } E[C_{i,j}] = (\\lambda_j - 1)D_{i,j-1}$$\n",
    "$$Var(D_{i,j}) = Var(C_{i,j}) = \\varphi \\lambda_j(\\lambda_j - 1)D_{i,j-1}$$\n",
    "\n",
    "##### Advantage\n",
    "- Mean is exactly the same as the CL method.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normal Approximation to the Negative Binomial Model\n",
    "\n",
    "$$E[D_{i,j}] = \\lambda_j D_{i,j-1} \\text{ and } E[C_{i,j}] = (\\lambda_j - 1)D_{i,j-1}$$\n",
    "$$Var(D_{i,j}) = Var(C_{i,j}) = \\varphi_j D_{i,j-1}$$\n",
    "\n",
    "#### Disadvantage\n",
    "- Additional parameters must be estimated in order to calculate the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Mean squared error of prediction (MSEP)/ prediction variance = process variance + parameter variance\n",
    "- $\\sqrt{\\text{MSEP}}$ = Root mean squared error of prediction (RMSEP)\n",
    "\n",
    "\n",
    "Standard error vs prediction error\n",
    "- The std. error = $\\sqrt{\\text{parameter variance}}$\n",
    "- prection error = $\\sqrt{\\text{process variance + parameter variance}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
