{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<b>Purpose</b>: Authors showcase various stochastic models where the chain-ladder reserve happens to be the maximum likelihood forecast of the true loss reserve.\n",
    "\n",
    "- All of the stochastic models outlined in this paper are generalized linear models (GLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Taylor's Cape Cod Method\n",
    "\n",
    "- The authors calculate the ELR differently.\n",
    "- ELR = Weighted expected loss ratio for each individual AY. We use CL method to project losses into future and then use percent development as weights.\n",
    "<center><img src = 'images/Taylor_CC_1.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src = 'images/Taylor_CC_2.JPG'></center>\n",
    "<center><img src = 'images/Taylor_CC_3.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CC Summary\n",
    "\n",
    "- Complete the triangle using the standard CL method.\n",
    "- Then calculate the weighted loss ratios. - Becomes yours ELR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exponential Dispersion Family\n",
    "\n",
    "<br>\n",
    "<center>$ln(\\pi(y;\\theta,\\phi)) = \\frac{y \\cdot \\theta - b(\\theta)}{a(\\phi)} + c(y,\\phi)$</center>\n",
    "\n",
    "- $a(\\phi) = \\frac{\\phi}{\\omega}$ and $\\omega$ = 1.\n",
    "<br><br>\n",
    "- E[Y] = $\\mu = b'(\\theta)$\n",
    "<br><br>\n",
    "- Var(Y) = $a(\\phi)b''(\\theta) = a(\\phi)V(\\mu)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tweedie Distribution\n",
    "\n",
    "<br>\n",
    "$$\\mu = [\\theta(1-p)]^{\\frac{1}{1-p}}$$<br>\n",
    "$$V(\\mu) = \\mu^p$$\n",
    "\n",
    "- Different distributions for various p values:\n",
    "    - p = 0 $\\rightarrow$ Normal\n",
    "    - p = 1 $\\rightarrow$ ODP\n",
    "    - p = 2 $\\rightarrow$ Gamma\n",
    "    - p = 3 $\\rightarrow$ Inverse Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The choice of p is informed by the heaviness of the tail indicated by the data. \n",
    "    - The tail heaviness increases with the value of p.\n",
    "    - Can look at dispersion of residuals to gauge if p value needs to increase.\n",
    "<br><br>    \n",
    "- Note, ODP is useful when little is known of the subject distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GLM vs Linear Regression\n",
    "\n",
    "- Weighted Linear(1) regression assumes we have an identity link function.\n",
    "    - Errors are normally distributed with unequal variances.\n",
    "<br><br>\n",
    "$$Y_i = x_i^T \\beta + \\epsilon_i,\\text{ where }\\epsilon_i \\sim N(0,\\phi_i)\\tag{1}$$\n",
    "<br>\n",
    "- GLM assumes link function is different from identity function AND non-normal errors.\n",
    "    - General LMs have normal errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Covariates - The independent variables used in the models.\n",
    "\n",
    "#### Goodness of Fit\n",
    "- We can use <b>unscaled deviance</b> to measure goodness-of-fit. Defined as:\n",
    "<br><br>\n",
    "<center>Deviance = 2($ll_{saturated} - ll_{model}$)</center>\n",
    "<br>\n",
    "- Deviance can also be used to measure scale parameter($\\phi$):\n",
    "\n",
    "$$\\hat{\\phi} = \\frac{D^*(Y,\\hat{Y})}{n-p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We also use <b>standardized Pearson residuals</b> to measure the goodness-of-fit. Well fitting models should have residuals that are unbiased and homoscedastic.\n",
    "    - They reproduce any non-normality that exists in the observations.\n",
    "<br><br>\n",
    "$$R_i^P = \\frac{Y_i-\\hat{Y_i}}{\\hat{\\sigma_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Since model assessment is easier when we have normally distributed residuals, it is common to look at <b>standardized deviance residuals</b> when assessing a GLM.\n",
    "<br><br>\n",
    "$$R_i^D = sgn(Y_i - \\hat{Y_i})\\sqrt{\\frac{d_i}{\\hat{\\phi}}}$$\n",
    "<br>\n",
    "<center>$ sgn =\n",
    "\\begin{cases}\n",
    "-1,  & \\text{if quantity is negative} \\\\\n",
    "0, & \\text{if quantity is 0} \\\\\n",
    "1, & \\text{if quantity is positive}\n",
    "\\end{cases}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Miscellaneous \n",
    "\n",
    "- We can also use weights to correct for heterscedasticity in the residuals.\n",
    "    - Select weights to be inverse of the variance.\n",
    "- We can remove influence of outliers by assigning them 0 weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-Parametric Mack Model\n",
    "\n",
    "- Same assumptions as in Mack 1994.\n",
    "\n",
    "#### Mack's Results\n",
    "1. The conventional CL estimators $\\hat{f}$ are unbiased and minimum variance estimators among estimators that are unbiased linear combinations of the $\\hat{f}$.\n",
    "2. Conventional CL estimator $\\hat{R}$ is unbiased.\n",
    "\n",
    "<br><br>\n",
    "Note: Mack model is stochastic because it considers mean and variance of observations. It is non-parametric because it does not consider the distribution of the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parametric Mack Models\n",
    "\n",
    "- All models below share assumptions from Mack 1994 paper except for the varaince assumption.\n",
    "\n",
    "\n",
    "#### EDF Mack Model\n",
    "- Next incr. loss given cum. loss is distributed according to EDF($Y_{k,j+1}|X_{kj} \\sim EDF$). \n",
    "\n",
    "#### Tweedie Mack Model\n",
    "- Next incr. loss given cum. loss is distributed according to Tweedie($Y_{k,j+1}|X_{kj} \\sim Tweedie$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### ODP Mack Model\n",
    "- Next incr. loss given cum. loss is distributed according to ODP($Y_{k,j+1}|X_{kj} \\sim ODP$).\n",
    "- This is special case of EDF model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Classified Models\n",
    "\n",
    "#### EDF Cross-Classified Model\n",
    "- Assumptions:\n",
    "    - Assumes random variables $Y_{k,j}$ (incremental losses) are stochastically independent.\n",
    "    - $Y_{k,j} \\sim EDF$\n",
    "    - $E[Y_{k,j}] = \\alpha_k \\cdot \\beta_j$\n",
    "    - $\\sum \\beta_j = 1$\n",
    "\n",
    "#### Cross-Classified vs Mack Model\n",
    "\n",
    "- Cross-classified model has an explicit parameter for row (alpha), while Mack model implicitly includes row parameter through conditioning on accumulated losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### ODP Cross-Classified Model\n",
    "\n",
    "- Assumptions\n",
    "    - Same assumptions as EDF cross-classified except $Y_{k,j} \\sim ODP$.\n",
    "    - The dispersion parameters are identical for all cells ($\\phi_{k,j} = \\phi$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/ODP_CC_1.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/ODP_CC_2.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/ODP_CC_3.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theorem 3.1\n",
    "\n",
    "- If we use EDF Mack model with the variance assumption from Mack model then we get the following:\n",
    "    - MLE estimators of $\\hat{f_i}$ are the standard CL estimators. (which are unbiased)\n",
    "    - For ODP Mack Model (special case of EDF Mack Model) and dispersion parameters based on just the column, then the standard CL estimators are minimum variance unbiased estimators (MVUEs).\n",
    "    - In addition, cumulative loss estimates and reserve estimates are also MVUEs.\n",
    "        - These estimators have minimum variance out of all unbiased estimators, not just the linear combinations of $\\hat{f}$ (under Mack model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theorem 3.2\n",
    "\n",
    "- Under the ODP cross-classified and EDF cross-classified models (as specified previously), the MLE fitted values and forcasts $\\hat{f}_{k,j}$ that are the same as those given by the standard CL method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theorem 3.3\n",
    "\n",
    "- In general, the MLEs $\\hat{Y}_{k,j}$ will not be unbiased. However, if we assume that the ODP cross-classified model assumptions apply AND that the fitted values and forecasts $\\hat{Y}_{k,j}$ and $\\hat{R}_k$ are corrected for bias, then they are MVUEs of $Y_{k,j}$ and $R_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Consequences of Theorem 3.2 and 3.3\n",
    "\n",
    "- Forecasts from the ODP Mack and ODP cross-classified models are identical and the same as those from the standard CL method despite the different formulations.\n",
    "- Forecasts can be obtained from the ODP cross-classified model without any explicit consideration of its parameters by working as if the model were the ODP Mack model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GLM Representation of ODP Mack Model\n",
    "\n",
    "<br>\n",
    "<center><img src='images/GLM_ODP_Mack.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GLM Representation of ODP Cross-Classified Model\n",
    "\n",
    "<br>\n",
    "<center><img src='images/GLM_ODP_CC.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Since this is over-parameterized, the GLm ssoftware will drop off one of the variables and the parameters from the GLM model will not match the non-GLM models.\n",
    "    - Can be solved by re-normalizing the parameters."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
