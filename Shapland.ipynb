{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The GLM Model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- GLM model uses incremental losses and log-link function\n",
    "\n",
    "$$E[q(w,d)] = m_{w,d}$$\n",
    "$$Var(q(w,d)) = \\phi m_{w,d}^z$$\n",
    "\n",
    "where, $m_{w,d} = e^{c + \\alpha_w + \\beta_2 + \\cdots + \\beta_d}$ = $e^{\\alpha_w + \\beta_2 + \\cdots + \\beta_d}$\n",
    "\n",
    "- Alpha adjusts for level and beta adjusts for development trends\n",
    "\n",
    "- In specification with constant (c), $\\alpha_1$ = $\\beta_1$ = 0\n",
    "- In the other specification, $\\alpha_1$ > 0 and $\\beta_1$ = 0 $\\leftarrow$ used in paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Power of z used to specify the error distribution:\n",
    "    - Normal: z = 0\n",
    "    - ODP: z = 1\n",
    "    - Gamma: z = 2\n",
    "    - Inverse Gaussian: z = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The ODP Model\n",
    "\n",
    "- An <b>important property</b> of the ODP model is that the future fitted incremental losses will equal the incremental claims from the standard CL model. (If we start with the diagonal and divide by LDFs to fill out the upper triangle)\n",
    "\n",
    "#### Three consequences of this property\n",
    "\n",
    "1. Simple link ratio algorithm can be used in place of the more complicated GLM algorithm, while still maintaining the GLM framework.\n",
    "2. Use of age-to-age factors acts as a bridge to the deterministic framework. This allows the model to be more easily explained.\n",
    "3. In general, the log-link function does not work for negative incremental claims. Using link ratios remedies this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Advantage of ODP Bootstrap\n",
    "\n",
    "- We assume that the residuals are <b>independent and identically distributed</b>, but do NOT require them to be normally distributed.\n",
    "\n",
    "#### Degrees of Freedom Adjustment Factor\n",
    "\n",
    "<center>$f^{DoF} = \\sqrt{\\frac{N}{N-P}}$</center>\n",
    "\n",
    "where, N = # of data cells in triangle and p = 2(# of AYs) -1\n",
    "\n",
    "- The <b>scaled Pearson residual</b> can be calculated as: $r_{w,d}^S = r_{w,d} * f^{DoF}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- When using standardized residuals, the scale parameter should still be calculated using the unscaled Pearson residuals. However, the scale parameter can be <b>approximated</b> as:\n",
    "\n",
    "$$\\phi^H = \\frac{\\sum(r_{w,d}^H)^2}{N}$$<br>\n",
    "\n",
    "Here's the equation for the regular scale parameter: $\\phi = \\frac{\\sum r_{w,d}^2}{N-P}$\n",
    "\n",
    "#### Process Variance\n",
    "\n",
    "- To include process variance in the future incremental losses, we assume that each future incremental loss, $m_{w,d}$, is the mean of a gamma distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ODP  Bootstrap Algorithm Mechanics\n",
    "\n",
    "<table><tr><td><img src='images/Bootstrap_1.JPG'></td><td><img src='images/Bootstrap_2.JPG'></td></tr></table>\n",
    "<center><img src='images/Bootstrap_3.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/Bootstrap_4.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/Bootstrap_5.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/Bootstrap_6.JPG'></center>\n",
    "<center><img src='images/Bootstrap_7.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/Bootstrap_8.JPG'></center>\n",
    "<center><img src='images/Bootstrap_9.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/Bootstrap_10.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bootstrapping the Incurred Loss Triangle\n",
    "\n",
    "#### Approach 1\n",
    "\n",
    "- Run both paid and incurred data models. Then use the payment pattern from each iteration of the paid data model to convert the ultimate values from each corresponding incurred model iteration to develop paid losses by AY.\n",
    "\n",
    "<b>Advantage:</b> It allows us to use the case reserves to help predict the ultimate losses, while still focusing on the payment stream for measuring risk.\n",
    "\n",
    "<b>Possible improvement:</b> Inclusion of correlation between paid and incurred models (possibly in the residual sampling process). For example, if we want to compare iterations showing long payment streams with iterations showing high incurred results, we must consider correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Approach 2\n",
    "\n",
    "- Apply the ODP bootstrap to the Munich Chain-Ladder (MCL) model. The MCL uses the inherent relationship/correlation between paid and incurred losses to predict ultimate  losses. When paid losses are low relative to incurred losses, then future paid loss development tends to be higher than average and vice versa.\n",
    "\n",
    "##### Advantages\n",
    "\n",
    "1. It does not require us to model paid losses twice.\n",
    "2. It explicitly measures the correlation between paid and incurred losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bootstrapping the BF and CC Models\n",
    "\n",
    "- To reduce the variability in the last few AYs (due to the application of MORE age-to-age factors), we can extrapolate using the BF or CC methods.\n",
    "- These methods can be made stochastic by assuming a distribution for the a priori loss ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GLM vs ODP Bootstrap Model\n",
    "\n",
    "- ODP bootstrap requires a parameter for each AY and development factor, while the GLM bootstrap requires fewer parameters.\n",
    "- GLM bootstrap does not use age-to-age factors, instead it relies on calculated parameter values to calculate future loss development.\n",
    "\n",
    "#### Drawbacks of GLM Bootstrap\n",
    "\n",
    "- The GLM must be solved for each iteration of the bootstrap model, which slows down the simulation.\n",
    "- The model is no longer easily explanable to others using age-to-age factors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Benefits of GLM Bootstrap\n",
    "\n",
    "- Fewer parameters help avoid over-parameterization of the model.\n",
    "- Allows us to include CY trend parameter(s) in the model.\n",
    "- Gives us the ability to model data shapes other than triangles.\n",
    "    - Can model trapezoid-shaped data\n",
    "- Allows us to match the model parameters to the statistical features found in the data, and extrapolate those features.\n",
    "    - Can get a tail factor by assuming last beta parameter continues past the end of the triangle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Issue Mentioned by the Author\n",
    "\n",
    "1. Negative incremental values\n",
    "2. Negative values during simulation\n",
    "3. Negative values and extreme outcomes\n",
    "4. Non-Zero sum of residuals\n",
    "5. Using L-year weighted average\n",
    "6. Missing values\n",
    "7. Outliers\n",
    "8. Heteroscedasticity\n",
    "9. Heteroecthesious data\n",
    "10. Exposure adjustment\n",
    "11. Tail factors\n",
    "12. Fitting a distribution to ODP bootstrap residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Issues - Negative incremental losses\n",
    "\n",
    "- Log-link function can't handle negative incremental values.\n",
    "- If the column-sum of incremental values is positive for GLM Bootstrap, then we can do the following:\n",
    "<br><br>\n",
    "<center> ln[q(w,d)] for q(w,d) > 0,</center>\n",
    "<center> 0 for q(w,d) = 0,</center>\n",
    "<center> -ln[abs|q(w,d)|] for q(w,d) < 0 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- If the column-sum of incremental values is negative, then we must do the following:\n",
    "\n",
    "<center><img src='images/Neg_Incr_1.JPG'></center>\n",
    "<center><img src='images/Neg_Incr_2.JPG'></center>\n",
    "<center><img src='images/Neg_Incr_3.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/Neg_Incr_4.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Issues - Negative Values During Simulation\n",
    "\n",
    "- When each future incremental value is sampled from a gamma distribution to add process variance, the parameters must be positive.\n",
    "- To simulate negative values when using a gamma distribution, we use one of the following two options:\n",
    "\n",
    "<center>$m_{w,d} = -Gamma[abs(m_{w,d}),\\phi \\cdot abs(m_{w,d})]$</center>\n",
    "<center>$m_{w,d} = Gamma[abs(m_{w,d}),\\phi \\cdot abs(m_{w,d})] + 2m_{w,d}$</center>\n",
    "\n",
    "- The second option is preferred since the resulting distribution will be skewed to the right (as it should be for a gamma distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table><tr><td><img src='images/Gamma_1.JPG'></td><td><img src='images/Gamma_2.JPG'></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Issues - Negative Values and Extreme Outcomes\n",
    "\n",
    "- Negative incremental values in the first few development columns can cause extreme outcomes.\n",
    "    - Can lead to large LDFs if the starting column has column-sum close to 0.\n",
    "    \n",
    "#### Ways of dealing with the issue\n",
    "1. <b>Identify extreme iterations and remove them</b> - Need to be careful and remove unreasonable iterations only.\n",
    "2. <b>Recalibrate the model</b> - Identify the source of the negative incremental losses and remove if necessary.\n",
    "    - If caused by sparse data in first row then, remove first row and reparameterize.\n",
    "    - If caused by sal/sub then model gross and sal/sub separately and then combine assuming 100% correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. <b>Limit incremental losses to zero</b> - Can be done to the original triangle, sampled triangle or the projected future incremental losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Issues - Non-Zero Sum of Residuals\n",
    "\n",
    "- Only adjust if we think Non-Zero sum of residuals is not reasonable. \n",
    "    - Non-Zero residuals could just be characteristic of the data set.\n",
    "- Simply add a constant to all the residual such that the sum of the shifted residuals is zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Issues - Using L-year Weighted Average\n",
    "\n",
    "#### GLM Bootstrap\n",
    "- Just use the relevant data from the triangle to parameterize the GLM model.\n",
    "\n",
    "#### ODP Bootstrap\n",
    "- Use L-year development factors for calculating incremental losses.\n",
    "- Exclude first few diagonals when calculating residuals.\n",
    "    - However, we still need to sample residuals for these diagonals ( need them for cum. loss calcs)\n",
    "- Using L-year average factors to project losses to ultimate.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Issues - Missing Values\n",
    "\n",
    "#### ODP Bootstrap\n",
    "\n",
    "- Estimate the missing values using surrounding values.\n",
    "- Exclude the missing value when calculating LDFs\n",
    "- However, still need to sample residuals for the missing values (need them for calculating cumulative values)\n",
    "- Must exclude missing value to calculate LDFs for projecting losses to ult.\n",
    "    - We want to maintain the uncertainty related to the missing value.\n",
    "- If missing values on the latest diagonal then we have 2 options:\n",
    "    1. Estimate the missing value.\n",
    "    2. Use the value in the second latest diagonal to construct the fitted triangle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### GLM Bootstrap\n",
    "\n",
    "- If desired, we can estimate the missing value using surrounding values.\n",
    "- Else, Parameterize the GLM model using the remaining values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Issues - Outliers\n",
    "\n",
    "- If the extreme value(s) are not representative of the variability of the dataset, then their impact could be removed from the model. (see solutions below)\n",
    "    \n",
    "#### ODP Bootstrap\n",
    "\n",
    "- Treat the outlier as missing value.\n",
    "- Exclude the outlier when calculating LDFs and residuals.\n",
    "    - Can be done in 3 ways:\n",
    "        1. Exclude in the numerator\n",
    "        2. Exclude in the denominator\n",
    "        3. Exclude in the numerator and denominator\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- However, still need to sample residual for the outlier (need them for calculating cumulative values)\n",
    "- Include the outlier cell to calculate LDFs for projecting losses to ult. (different from missing value)\n",
    "    - This way we include some non-extreme variability in the sample triangle projection.\n",
    "    \n",
    "#### GLM Bootstrap\n",
    "- Treat the outliers as missing data and parameterize the model with the remaining data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What to do if there are a significant number of Outliers?\n",
    "\n",
    "1. Might indicate that the model is a poor fit to the data.\n",
    "2. <b>GLM bootstrap</b> - New parameters could be chosen or the error distribution could be changed (i.e z parameter)\n",
    "3. <b>ODP bootstrap</b> - Since ODP does not require a distribution for the residuals, it could be the case that the residuals are highly skewed. If skewness is real, then the outliers should be included. L-year wtd. average could also be used to provide a better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Issues - Heteroscedasticity\n",
    "\n",
    "- The regular ODP bootstrap method works because we assume all residuals are independent and identically distributed, if this is not the case then we need to make adjustments to the procedure.\n",
    "\n",
    "- If the standardized residuals have different variances then we have <b>heteroscedasticity</b>.\n",
    "    - Need to mindful of credibility when making assessment. (especially in the tail)\n",
    "    \n",
    "#### Stratified Sampling\n",
    "- Group development periods with homogeneous variances.\n",
    "- Sample with replacement from the residuals within each group separately.\n",
    "- <b>Advantage</b> - It is simple to understand and implement.\n",
    "- <b>Disadvantage</b> - Some groups may have few residuals in them, which limits the amount of variability in the possible outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Using Variance Parameter\n",
    "- Calculate hetero-adjustment factor for each group, i:\n",
    "\n",
    "$$h_i = \\frac{stdev(\\cup^{Total} r_{w,d}^H)}{stdev(\\cup_i r_{w,d}^H)}$$\n",
    "\n",
    "<center><img src='images/Hetero_1.JPG'></center>\n",
    "\n",
    "- Basically adjust the residuals so the whole triangle has similar variance/std.\n",
    "- After sampling, we adjust the residual to the variance/std. of the new cohort. This means the $h_i$ below may be different from the $h_i$ used initially.\n",
    "<center><img src='images/Hetero_2.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Using Scale Parameter\n",
    "\n",
    "- Calculate the scale parameter for each group. \n",
    "\n",
    "<center><img src='images/Hetero_3.JPG'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- After this, similar to the variance parameter case, adjust variance based on the new cohort.\n",
    "- Note, the residuals being used in this case are the <b>unscaled pearson residuals</b>.\n",
    "- Also note that hetero adjustment factors are considered as a additional parameter (subtract 1).\n",
    "    - p = p + #(hetero groups) -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Issues - Heteroecthesious Data\n",
    "\n",
    "- This is when you have an incomplete or uneven exposures at interim evaluation dates.\n",
    "    - Partial first development period - eval period is at 6, 18, 30, etc. mos.\n",
    "    - Partial last calendar period - latest eval has 6, 18, 30, etc. mos of dev.\n",
    "    \n",
    "#### Partial First Development Period Data\n",
    "\n",
    "- Not a problem for parameterizing an ODP bootstrap model since Pearson residuals use the square root of the fitted value to make them all \"exposure independent\"\n",
    "    - Units cancel out when we calculate residuals\n",
    "- In <b>deterministic analysis</b>, for most recent AY, we simply reduce the projected future payments by half to remove the exposures from 06/30 to 12/31."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- During <b>ODP bootstrap simulation process</b>, for most recent AY, again reduce the projected future values by half, then simulate the process variance as usual.\n",
    "    - Alternatively, we can reduce the future values by half AFTER simulating the process variance.\n",
    "    \n",
    "#### Partial Last Calendar Period Data\n",
    "\n",
    "- <b>Deterministic analysis</b> - exclude the last diagonal when calculating age-to-age factors, interpolate those factors for the exposures in the last diagonal and use them for projecting future losses. Then reduce the future values for the latest AY by half.\n",
    "\n",
    "- <b>Parameterizing ODP bootstrap model</b> - Annualize the exposures in the last diagonal to make it consistent with the rest of the triangle. Then calculate fitted triangle and residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- <b> ODP bootstrap simulation process</b> - After getting sample triangle, calculate LDFs. Then adjust latest diagonal back to 6 mos. Then multiply cumulative values by interpolated age-to-age factors to project future losses. Then reduce the future values for the latest AY by half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Issues -  Exposure Adjustment\n",
    "\n",
    "- Has to be done when exposures change dramatically over the years.\n",
    "\n",
    "#### ODP Bootstrap Model\n",
    "- Divide losses by earned exposures to get pure premiums.\n",
    "- At the end multiply the result with corresponding earned exposures.\n",
    "\n",
    "#### GLM Bootstrap Model\n",
    "- Fit on pure premiums as well.\n",
    "- However, we assume AYs with higher exposures have lower variance and vice  versa.\n",
    "- This could allow fewer AY parameters for the GLM bootstrap model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Issues -  Tail Factors\n",
    "\n",
    "#### ODP Bootstrap Model\n",
    "- Can add a tail factor to each sample triangle.\n",
    "- Could extrapolate age-to-ult factor over 3 years using a decay model with decay of 50%. 60-ult: 1.05, 72-ult: 1.025(1 + .05x.5), 84-ult: 1.0125(1 + .025x.5)\n",
    "- To make it a stochastic process, could assume a distribution for tail factors.\n",
    "\n",
    "#### GLM Bootstrap Model\n",
    "- The last beta parameter essentially gives us the tail factor - we assume the last beta parameter applies incrementally until it's effect on the future incremental losses is negligible.\n",
    "- Same logic if we also have CY parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Issues -  Fitting Distribution to ODP Bootstrap Residuals\n",
    "\n",
    "- If we believe that extreme observations are NOT captured well in the loss triangle, then we can parameterize a distribution for the residuals (such as normal) and resample using the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics Mentioned By the Author\n",
    "\n",
    "1. Residual graphs\n",
    "2. Normality test\n",
    "3. Outliers\n",
    "4. Parameter adjustment\n",
    "5. Estimate unpaid model results\n",
    "6. Mean and std. of incremental values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics - Residual Graphs\n",
    "\n",
    "- Test the assumption that reiduals are independent and identically distributed.\n",
    "- Types of residual graphs\n",
    "    - Development Period\n",
    "    - Accident Period\n",
    "    - Calendar Period\n",
    "    - Incremental Losses\n",
    "- The residuals should have a random pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics - Normality Test\n",
    "\n",
    "- Compare ODP model residuals against normal dist to compare parameter sets and assess skewness.\n",
    "\n",
    "<center><img src='images/Normality_Plot.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Calculate Test Values\n",
    "\n",
    "- <b>P-value</b> - If the residuals are normally distributed, the p-value should be greater than 5%.\n",
    "- $R^2$ - The value for normally distributed residuals should be close to 1.\n",
    "- $R^2$ and <b>p-value</b> are criticized for not penalizing the model for the number of parameters.\n",
    "\n",
    "- <b>AIC</b> and <b>BIC</b> penalize for parameters used. We want the values to be as small as possible.\n",
    "\n",
    "$$AIC = 2p + n \\left[ln\\left(\\frac{2 \\pi RSS}{n}\\right) + 1 \\right]$$<br>\n",
    "$$BIC = n*ln\\left(\\frac{RSS}{n}\\right) + p*ln(n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics - Outliers\n",
    "\n",
    "- Can identified using box-whisker plot\n",
    "- Only adjust/remove outliers that are unreasonable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics - Parameter Adjustment\n",
    "\n",
    "- For the <b>GLM Model</b> - we keep adding parameters to the \"basic\" model as long as residuals are not randomly distributed around 0 (For various cuts of data).\n",
    "    - The implied pattern for GLM model should be a smoothed version of the CL/ODP model.\n",
    "<br><br>    \n",
    "<center><img src='images/ODPvsOptimal_GLM.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics - Estimated Unpaid Model Results\n",
    "\n",
    "- The standard error should increase when moving from the oldest years to the most recent years\n",
    "    - This is because standard error follows the magnitude of the results\n",
    "- The total standard error should be larger than any individual error.\n",
    "- The coefficient of variation (C.V.) should generally decrease when moving from the oldest to the most recent years.\n",
    "    - C.V. for all years combined should be less than any inidividual AY.\n",
    "    - Oldest AYs have the least parameters and the greatest uncertainty\n",
    "    - For Recent AYs, random variations in the remaining payments tend offset one another, thereby reducing variability. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- However, CV may rise in the most recent AYs due to:\n",
    "    - With the increasing # of params., the param. uncertainty may overpower the process uncertainty, causing an increase in variability.\n",
    "    - The model may simply be overestimating the variability in the most recent years.\n",
    "        - Might have to use BF or CC method in this case for latest AYs. \n",
    "- Total CV should be smaller than any individual year's CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics - Mean and Std. of Incremental Values\n",
    "\n",
    "- The mean and std. of future values should help narrow down issues with the CV for the AY.\n",
    "\n",
    "<center><img src='images/CV.JPG'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimated CY Cash Flow Results\n",
    "\n",
    "- For CY, standard errors decrease and coefficient of variation increase as we move from older to more recent years.\n",
    "    - You are moving 1 diagonal to the right when moving to newest CY.\n",
    "    - As we move further out in the future, the CY unpaid claim estimates will decrease, which also lead to a decrease in size of standard errors.\n",
    "    - This is opposite of the AY results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using Multiple Models\n",
    "\n",
    "#### Run Models With the Same Random Variables\n",
    "- Random residuals are picked in the same order.\n",
    "- The incremental values for each model are weighted together.\n",
    "- This method of combining does cause correlation in the model results since each model is run using the same set of random residuals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Run Model with Independent Random Variables\n",
    "- Random residuals are picked randomly for each model.\n",
    "- Weights are used to select a model by randomly sampling iterations from each model.\n",
    "    - For 1000 iterations and weights of 25% and 75%, we sample 250 iterations from the first model and 750 iterations from the second model.  \n",
    "<br><br>\n",
    "- <b>Note:</b> weighting may produce results by AY, such as negative IBNR, if negative IBNR is not reasonable then the distributions can be adjusted in following manner: \n",
    "    - Shape and width of dist. are appropriate, then fixed amt can be added to produce positive IBNR.\n",
    "    - Shape or width is not appropriate, then a factor can be multiplied to produce positive IBNR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Smooth Unpaid Losses\n",
    "\n",
    "- We can also fit distributions to unpaid losses to generate smooth results.\n",
    "- The smoothed results can be used to:\n",
    "    - Assess the quality of the fit\n",
    "    - Parameterize a dynamic financial analysis (DFA) model\n",
    "    - Estimate extreme values\n",
    "    - Estimate TVaR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Correlation\n",
    "\n",
    "- We can aggregate the results for each LOB up to the portfolio level.\n",
    "- Since business segments tend to be correlated, we need to consider this correlation when calculating portfolio results.\n",
    "\n",
    "#### Location Mapping\n",
    "- Pick the residuals in the same order for all the LOBs.\n",
    "- <b>Advantage</b> - Can be easily implemented in a spreadsheet and does not require us to estimate a correlation matrix.\n",
    "- <b>Disadvantages</b> \n",
    "    - Requires all LOBs to have residual triangles of same size with no missing values or outliers. \n",
    "    - Correlation of original residuals is used in the model, and can't use another correlation assumption for stress testing the aggregate results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Re-sorting\n",
    "\n",
    "- The residuals are re-sorted until the rank correlation between LOBs matches the desired correlation.\n",
    "    - P-value, correlation coefficient for LOBs and judgement can be used to select correlation matrix assumption.    \n",
    "- <b>Benefits</b>\n",
    "    - Different correlation assumptions may be employed.\n",
    "        - Might have beneficial impacts on aggregate distribution\n",
    "    - Can be used with triangles of different shapes and sizes."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
